# Adobe Customer Journey Analytics Solution Design Reference Generator with Data Quality Validation

<img width="1024" height="572" alt="image" src="https://github.com/user-attachments/assets/54a43474-3fc6-4379-909c-452c19cdeac2" />


**Version 3.0.4** - A production-ready Python tool for auditing your Customer Journey Analytics (CJA) implementation by generating comprehensive Solution Design Reference (SDR) documents with enterprise-grade data quality validation, high-performance batch processing, automatic retry with exponential backoff, and modern dependency management.

## What Makes Version 3.0 Different

This tool evolved from a Jupyter notebook proof-of-concept into a production-ready, enterprise-grade automation solution. Building on the foundation established in the [CJA Summit 2025 notebook](https://github.com/pitchmuc/CJA_Summit_2025/blob/main/notebooks/06.%20CJA%20Data%20View%20Solution%20Design%20Reference%20Generator.ipynb), Version 3.0 represents a comprehensive enterprise solution with significant performance and usability improvements:

### High-Performance Batch Processing

**From Single Data View to Enterprise-Scale Batch Operations**
- **3-4x Performance Improvement**: Process multiple data views in parallel using true multiprocessing
- **ProcessPoolExecutor**: Leverages separate processes for CPU-bound operations (no Python GIL limitations)
- **Configurable Parallelism**: Adjust worker count (default: 4) based on your infrastructure and API limits
- **Continue-on-Error**: Optional resilience mode processes all data views even if some fail
- **Comprehensive Results Tracking**: Detailed success/failure reporting with timing metrics

**Performance Comparison:**
```
Sequential: 10 data views Ã— 35s = 350 seconds (5.8 minutes)
Parallel:   10 data views / 4 workers Ã— 35s = ~87.5 seconds (1.5 minutes)
Result:     4x faster (75% time savings)
```

### Command-Line Interface

**Professional CLI with argparse**
- **Required Arguments**: Data view IDs must be explicitly provided (no hardcoded defaults)
- **Flexible Options**: Batch mode, worker count, output directory, log level, and more
- **Comprehensive Help**: Built-in `--help` with usage examples and documentation
- **Input Validation**: Automatic validation of data view ID format
- **Error Messages**: Clear, actionable error messages with suggested fixes

**Usage Examples:**
```bash
# Single data view
uv run python cja_sdr_generator.py dv_12345

# Multiple data views (parallel batch processing)
uv run python cja_sdr_generator.py --batch dv_12345 dv_67890 dv_abcde

# Custom configuration
uv run python cja_sdr_generator.py --batch dv_* --workers 8 --output-dir ./reports
```

### Modern Python Tooling with UV

**From Jupyter Notebook to Production Application**
- **Reproducible Builds**: Lock file (`uv.lock`) ensures identical dependency versions across all environments
- **Lightning-Fast Installation**: UV's Rust-based resolver installs packages 10-100x faster than pip
- **Standardized Configuration**: `pyproject.toml` follows PEP 518/621 standards for modern Python projects
- **Zero Configuration Conflicts**: UV's advanced resolver eliminates dependency issues
- **Professional Project Structure**: Clear separation of concerns with proper package management

**Why UV Over Traditional pip?**
- **Speed**: Package resolution in milliseconds vs seconds/minutes
- **Reliability**: Deterministic builds with cryptographic lock files  
- **Simplicity**: One command (`uv sync`) replaces multiple pip operations
- **Modern**: Built for Python 3.14+ with future-proof architecture
- **Developer Experience**: Better error messages, faster iteration cycles

### Enterprise-Grade Reliability

**Comprehensive Error Handling & Validation**
- Pre-flight configuration validation before API calls
- Graceful degradation when partial data is unavailable
- Detailed error messages with actionable troubleshooting steps
- Automatic data view existence verification
- **Automatic retry with exponential backoff** for transient network failures
- Safe filename generation and permission handling

**Retry with Exponential Backoff**
- Automatic retry on ConnectionError, TimeoutError, and network issues
- Exponential backoff: 1s â†’ 2s â†’ 4s (configurable)
- Jitter randomization to prevent thundering herd problems
- Up to 3 retry attempts before failing (configurable)
- Non-retryable errors fail immediately (no wasted time)

**Production Logging System**
- Timestamped log files with rotation in dedicated `logs/` directory
- Dual output streams (console + file) for real-time monitoring and audit trails
- Structured logging with severity levels (INFO, WARNING, ERROR, CRITICAL)
- Performance metrics tracking for optimization
- Complete execution summaries for reporting

### Advanced Data Quality Validation

**Automated Quality Assurance**
Unlike the original notebook's simple data retrieval, Version 3.0 includes a comprehensive data quality framework:

- **8+ Validation Checks**: Duplicates, missing fields, null values, invalid IDs, empty datasets
- **Validation Caching**: Optional caching for 50-90% faster repeated validations
- **Parallel Validation**: Metrics and dimensions validated concurrently (10-15% faster)
- **Optimized Single-Pass Validation**: 30-50% faster validation using vectorized operations
- **Early Exit Optimization**: 15-20% faster on error scenarios with fail-fast behavior
- **Thread-Safe Design**: Lock-protected concurrent validation for reliable operation
- **Severity Classification**: CRITICAL, HIGH, MEDIUM, LOW with color-coded Excel formatting
- **Actionable Insights**: Detailed issue descriptions with affected component lists
- **Quality Dashboard**: Dedicated "Data Quality" sheet with filtering and sorting
- **Performance Tracking**: Built-in timing metrics and cache statistics
- **Trend Analysis Ready**: Consistent reporting format for tracking quality over time

**Quality Checks Include:**
1. Duplicate component name detection across metrics and dimensions
2. Required field validation (id, name, type)
3. Critical field null value identification
4. Missing description detection for documentation completeness
5. Invalid or malformed ID detection
6. Empty dataset handling and reporting
7. API response structure validation
8. Cross-referential integrity checks

**Optimized Validation Performance:**
```
Architecture:         Original â†’ Optimized â†’ Parallel + Cached
DataFrame Scans:      9 scans  â†’ 1 scan (89% reduction)
Validation Logic:     Sequential â†’ Vectorized â†’ Concurrent
Execution:            Serial   â†’ Serial    â†’ ThreadPoolExecutor
Caching:              None    â†’ None      â†’ Optional LRU cache
Performance Impact:   30-50% faster â†’ Additional 10-15% with parallel â†’ 50-90% with cache hits

Example (225 components):
  Before (v1.0): 2.5s validation time
  After (v3.0 optimized):  1.2s validation time (52% faster)
  After (v3.0 parallel):   1.0s validation time (60% faster overall)
  After (v3.0 cached hit): 0.1s validation time (96% faster overall)
  Result: Up to 2.4s saved per data view on cache hits
```

### From Script to Application

**Architectural Improvements**

| Aspect | Original Notebook | Version 3.0 |
|--------|------------------|-------------|
| **Execution Model** | Interactive cells | CLI with argparse |
| **Performance** | Single-threaded | **3-4x faster (parallel)** |
| **Processing Mode** | One at a time | **Batch processing** |
| **Parallelism** | None | **ProcessPoolExecutor (CPU)** |
| **CLI Arguments** | None | **Required arguments** |
| **Error Handling** | Basic try-catch | **Continue-on-error mode** |
| **Logging** | Print statements | **Batch + per-view logs** |
| **Dependencies** | Manual installation | Managed via pyproject.toml + uv |
| **Data Quality** | None | **8+ automated checks** |
| **Validation Performance** | N/A | **30-50% faster (optimized)** |
| **Configuration** | Hardcoded values | **CLI with validation** |
| **Reliability** | Single-run, manual | **Resilient batch mode** |
| **Maintainability** | Notebook-based | **Modular with workers** |
| **Scalability** | Single data view | **Unlimited data views** |
| **Output** | Basic Excel | **Multiple outputs in parallel** |
| **Result Tracking** | None | **Batch summary reports** |
| **Throughput** | ~1 view/35s | **~4 views/35s (4 workers)** |

### Enhanced Output & Documentation

**Flexible Output Formats**
- **Excel** (default): Professional formatted workbooks with color-coding and multi-sheet organization
- **CSV**: Individual CSV files for easy data processing and ETL pipelines
- **JSON**: Hierarchical structured data for API integration and automation
- **HTML**: Web-ready reports with modern styling for sharing and presentations
- **All**: Generate all formats simultaneously for comprehensive documentation packages

See [OUTPUT_FORMATS.md](OUTPUT_FORMATS.md) for detailed format specifications, use cases, and integration examples.

**Professional Excel Workbooks**
- **5 Formatted Sheets**: Metadata, Data Quality, DataView details, Metrics, Dimensions
- **Conditional Formatting**: Severity-based color coding for instant issue identification
- **Auto-filtering**: Every sheet has sortable, filterable columns
- **Frozen Headers**: Easy navigation through large datasets
- **Text Wrapping**: Readable long descriptions and JSON content
- **Alternating Rows**: Improved visual scanning

**Comprehensive Metadata**
- Generation timestamp with timezone
- Component type distributions and breakdowns  
- Data quality summary with issue counts by severity
- Version tracking for audit compliance
- API response metadata for troubleshooting

### Operational Excellence

**Built for Automation**
- Command-line friendly (batch scripts, cron jobs, CI/CD pipelines)
- Exit codes for monitoring and alerting systems
- Structured logs for log aggregation tools (Splunk, ELK, Datadog)
- Reproducible builds for containerization (Docker, Kubernetes)
- Environment variable support for secure credential management

**DevOps Ready**
```bash
# Single command deployment
uv sync && uv run python cja_sdr_generator.py dv_production_12345

# Scheduled execution (cron) - single data view
0 9 * * 1 cd /path/to/project && uv run python cja_sdr_generator.py dv_production_12345

# Scheduled execution (cron) - batch processing multiple data views
0 2 * * * cd /path/to/project && uv run python cja_sdr_generator.py --batch dv_prod_1 dv_prod_2 dv_prod_3 --continue-on-error

# CI/CD pipeline integration
- name: Generate SDR
  run: |
    uv sync
    uv run python cja_sdr_generator.py --batch dv_production_12345 dv_staging_67890 --continue-on-error
```

### Documentation & Support

**Comprehensive README**
- Detailed installation instructions for all platforms
- Troubleshooting guide with common error solutions
- Best practices for production usage
- Security guidelines for credential management
- Performance optimization tips
- Multiple usage examples and scenarios

### Who Should Use Version 3.0?

**Ideal For:**
- **Analytics Teams** requiring regular SDR documentation across multiple data views
- **DevOps Engineers** automating CJA audits in CI/CD pipelines with batch processing
- **Data Governance** teams needing audit trails and quality tracking at scale
- **Consultants** managing multiple client CJA implementations efficiently
- **Enterprise Organizations** with compliance and documentation requirements
- **Multi-Environment Teams** needing to process production, staging, and development data views
- **Large Organizations** with dozens or hundreds of data views requiring regular audits

**Migration Path from Notebook:**
The notebook version is excellent for learning and ad-hoc exploration. Version 3.0 is designed for teams that need:
- **High-Performance Processing**: 3-4x faster with parallel batch mode
- **Scheduled, Automated SDR Generation**: CLI-ready for cron jobs and task schedulers
- **Multi-Data View Support**: Process unlimited data views in parallel
- **Data Quality Monitoring**: Track quality trends across all environments
- **Reliable Execution**: Resilient batch mode with continue-on-error
- **Professional Documentation**: Generate comprehensive SDRs for stakeholders
- **Audit Trails**: Complete logging for compliance purposes
- **Scalability**: Handle enterprise-scale CJA implementations efficiently

---

## Table of Contents

* **1. Overview**
  * 1.1 What the Script Does
  * 1.2 Key Features
  * 1.3 What's New in This Version
* **2. Pre-requisites**
  * 2.1 System Requirements
  * 2.2 Required Libraries
  * 2.3 Authentication Setup
* **3. Installation**
  * 3.1 Installing uv Package Manager
  * 3.2 Project Setup
  * 3.3 Configuration File Setup
* **4. Usage**
  * 4.1 Basic Usage
  * 4.2 Configuration
  * 4.3 Running the Script
  * 4.4 Output Files
* **5. Output Structure**
  * 5.1 Excel Workbook Sheets
  * 5.2 Log Files
* **6. Data Quality Validation**
  * 6.1 Validation Checks
  * 6.2 Severity Levels
  * 6.3 Understanding Issues
* **7. Error Handling**
  * 7.1 Connection Errors
  * 7.2 Authentication Issues
  * 7.3 Data View Problems
  * 7.4 Network Errors and Automatic Retry
  * 7.5 Troubleshooting Guide
* **8. Use Cases**
* **9. Best Practices**
* **10. Testing**
* **11. Performance Optimizations**
  * 11.1 Optimized Data Quality Validation
* **12. Support and Logging**
* **13. Additional Resources**

---

## 1. Overview

### 1.1 What the Script Does

This enterprise-grade script audits your Customer Journey Analytics implementation by:

1. **Connecting to CJA**: Establishes secure API connection with comprehensive validation
2. **Validating Data Views**: Verifies data view existence and accessibility before processing
3. **Fetching Components**: Retrieves all metrics and dimensions from your specified Data View
4. **Quality Validation**: Performs 8+ automated data quality checks on your components
5. **Generating Documentation**: Creates a professionally formatted Excel workbook with multiple sheets
6. **Comprehensive Logging**: Tracks all operations with detailed logs for auditing and troubleshooting

### 1.2 Key Features

**Enterprise-Grade Error Handling**
- Validates configuration files before connection attempts
- Graceful failure handling with actionable error messages
- **Automatic retry with exponential backoff** for transient network failures
- Comprehensive logging to both console and file
- **Color-coded console output** for instant visual feedback

**Advanced Data Quality Validation**
- Duplicate detection across metrics and dimensions
- Required field validation
- Null value identification in critical fields
- Missing description detection
- Invalid ID detection
- Empty dataset handling
- Color-coded severity levels (CRITICAL, HIGH, MEDIUM, LOW)

**Professional Excel Output**
- Six formatted sheets with color-coded data quality issues
- Alternating row colors for readability
- Auto-filtering and frozen headers on all sheets
- Automatic column width adjustment
- Text wrapping for long content
- Severity-based conditional formatting

**Robust Logging System**
- Timestamped log files in dedicated `logs/` directory
- Dual output to console and file
- Detailed operation tracking
- Error diagnosis information
- Execution summaries

### 1.3 What's New in Version 3.0.3

**Latest Updates (v3.0.2 - v3.0.3):**

ðŸ”„ **Retry with Exponential Backoff** (v3.0.3)
- Automatic retry on transient network errors (ConnectionError, TimeoutError)
- Exponential backoff with jitter to prevent thundering herd
- Configurable: 3 retries, 1s base delay, 30s max delay
- Applied to all CJA API calls for maximum reliability

ðŸŽ¨ **CLI Quick Wins** (v3.0.2)
- `--version` flag to display program version
- `--quiet` / `-q` mode to suppress output except errors
- Color-coded console output (green=success, red=error, yellow=warning)
- Total runtime display in final summary
- Enhanced config validation with schema-based type checking

**Core Enterprise Features:**

ðŸš€ **High-Performance Batch Processing**
- Process multiple data views in parallel (3-4x faster)
- True multiprocessing with ProcessPoolExecutor (no GIL limitations)
- Configurable worker count (default: 4, adjustable from 1-8+)
- Continue-on-error mode for resilient batch operations
- Comprehensive batch summary reports with success/failure tracking

ðŸŽ¯ **Command-Line Interface**
- Required data view arguments (no hardcoded defaults)
- Full argument parsing with `argparse`
- `--batch` mode for parallel processing
- `--workers N` to control parallelism
- `--output-dir PATH` for custom output locations
- `--continue-on-error` for resilient processing
- `--log-level LEVEL` for logging control
- Built-in `--help` with comprehensive examples

ðŸ“Š **Enhanced Results Tracking**
- Detailed batch processing summaries
- Per-data-view timing and status
- Success rate calculations
- Throughput metrics (data views per minute)
- Individual and aggregate error reporting

**Modern Dependency Management**

- Uses `uv` for fast, reliable package management
- `pyproject.toml` for standardized project configuration
- Reproducible builds with lock files
- Python 3.14+ compatibility

**Comprehensive Error Handling**

- Pre-flight validation of configuration files
- API connection testing before data operations
- Graceful handling of network failures
- Detailed error messages with troubleshooting steps

**Data Quality Validation**

- Automated quality checks with detailed reporting
- **Validation caching**: 50-90% faster on cache hits with optional LRU cache
- **Parallel validation**: 10-15% faster through concurrent processing with ThreadPoolExecutor
- **Optimized single-pass validation**: 30-50% faster through vectorized operations
- **Early exit optimization**: 15-20% faster on error scenarios with fail-fast behavior
- **Logging optimization**: 5-10% faster with production mode
- **Thread-safe concurrent execution**: Lock-protected operations for reliability
- "Data Quality" sheet with color-coded issues
- Severity-based prioritization
- Performance tracking and cache statistics
- Actionable recommendations

**Advanced Logging**

- Timestamped log files with rotation
- Progress tracking for long operations
- Error diagnosis and stack traces
- Execution summary reporting
- Separate logs for batch and single mode

**Improved Reliability**

- Validates data view existence before processing
- Handles empty datasets gracefully
- Continues processing despite partial failures
- Safe filename generation

---

## 2. Pre-requisites

### 2.1 System Requirements

- **Python**: Version 3.14 or higher
- **uv**: Modern Python package manager (recommended)
- **Operating System**: Windows, macOS, or Linux
- **Disk Space**: Minimum 100MB for logs and output files
- **Network**: Internet connectivity to Adobe CJA APIs

### 2.2 Required Libraries

All dependencies are managed through `pyproject.toml`:

```toml
[project]
name = "cja-auto-sdr-2026"
version = "3.0.3"
description = "Customer Journey Analytics SDR Generator with Data Quality Validation"
readme = "README.md"
requires-python = ">=3.14"
dependencies = [
    "cjapy>=0.2.4.post2",
    "pandas>=2.3.3",
    "xlsxwriter>=3.2.9",
    "tqdm>=4.66.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.4",
]
```

**Core Dependencies:**
- `cjapy>=0.2.4.post2` - Customer Journey Analytics API wrapper
- `pandas>=2.3.3` - Data manipulation and analysis
- `xlsxwriter>=3.2.9` - Excel file generation with formatting
- `tqdm>=4.66.0` - Progress bar indicators for long-running operations
- `pytz` - Timezone handling (included with pandas)

### 2.3 Authentication Setup

You need a valid Adobe I/O integration with CJA API access:
- Organization ID
- Client ID (API Key)
- Technical Account ID
- Client Secret
- Private Key file

---

## 3. Installation

### 3.1 Installing uv Package Manager

`uv` is a fast Python package installer and resolver written in Rust. It's significantly faster than pip and provides better dependency resolution.

**macOS and Linux:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows (PowerShell):**
```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**Alternative (using pip):**
```bash
pip install uv
```

**Verify installation:**

```bash
uv --version
```

### 3.2 Project Setup

#### Option 1: Clone/Download Project (Recommended)

If you have the project directory with `pyproject.toml`:

```bash
# Navigate to project directory
cd cja-auto-sdr-2026

# Create virtual environment and install dependencies
uv sync

# Activate the virtual environment
# On macOS/Linux:
source .venv/bin/activate

# On Windows:
.venv\Scripts\activate
```

The `uv sync` command will:
- Create a virtual environment in `.venv/`
- Install all dependencies from `pyproject.toml`
- Generate a `uv.lock` file for reproducible builds

#### Option 2: Start from Scratch

If starting a new project:

```bash
# Create new project
uv init cja-auto-sdr-2026
cd cja-auto-sdr-2026

# Add dependencies
uv add cjapy>=0.2.4.post2
uv add pandas>=2.3.3
uv add xlsxwriter>=3.2.9

# Copy your script into the project directory
# Copy myconfig.json into the project directory
```

#### Option 3: Legacy pip Installation

If you prefer traditional pip:

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate

# Install dependencies
pip install cjapy>=0.2.4.post2 pandas>=2.3.3 xlsxwriter>=3.2.9
```

### 3.3 Configuration File Setup

Create a `myconfig.json` file in the project root directory. Two authentication methods are supported:

**Option 1: OAuth Server-to-Server (Recommended)**

```json
{
  "org_id": "your_org_id@AdobeOrg",
  "client_id": "your_client_id",
  "secret": "your_client_secret",
  "scopes": "openid, AdobeID, additional_info.projectedProductContext"
}
```

**Option 2: JWT (Legacy)**

```json
{
  "org_id": "your_org_id@AdobeOrg",
  "client_id": "your_client_id",
  "tech_id": "your_tech_account_id@techacct.adobe.com",
  "secret": "your_client_secret",
  "private_key": "path/to/private.key"
}
```

**Configuration Fields:**
- `org_id`: Your Adobe Organization ID (found in Adobe Developer Console)
- `client_id`: Client ID from your integration
- `secret`: Client Secret from your integration
- `scopes`: OAuth scopes (for OAuth Server-to-Server auth only)
- `tech_id`: Technical Account ID (for JWT auth only)
- `private_key`: Path to your private key file (for JWT auth only)

**Project Structure:**
```
cja-auto-sdr-2026/
â”œâ”€â”€ .venv/                    # Virtual environment (created by uv)
â”œâ”€â”€ logs/                     # Log files (created automatically)
â”œâ”€â”€ pyproject.toml           # Project configuration
â”œâ”€â”€ uv.lock                  # Dependency lock file (created by uv)
â”œâ”€â”€ myconfig.json            # Your CJA credentials (DO NOT COMMIT)
â”œâ”€â”€ private.key              # Your private key (DO NOT COMMIT)
â”œâ”€â”€ cja_sdr_generator.py     # Main script
â””â”€â”€ README.md                # This file
```

**Security Note:** Keep `myconfig.json` and `private.key` secure and never commit them to version control. Add them to your `.gitignore` file:

```gitignore
# .gitignore
myconfig.json
*.key
*.pem
.venv/
logs/
*.xlsx
```

---

## 4. Usage

### 4.1 Basic Usage

1. **Prepare Configuration**: Ensure `myconfig.json` is properly configured
2. **Get Data View IDs**: Identify which data view(s) you want to process
3. **Run the Script**: Execute with required data view ID arguments
4. **Review Output**: Check the generated Excel file(s) and log file(s)

**Quick Start:**
```bash
# Single data view
uv run python cja_sdr_generator.py dv_YOUR_ID

# Multiple data views (parallel batch processing)
uv run python cja_sdr_generator.py --batch dv_ID1 dv_ID2 dv_ID3
```

### 4.2 Command-Line Arguments

**Required Arguments:**
- `DATA_VIEW_ID [DATA_VIEW_ID ...]` - One or more data view IDs (must start with `dv_`)

**Optional Arguments:**
- `--version` - Show program version and exit
- `--batch` - Enable parallel batch processing mode
- `--workers N` - Number of parallel workers (default: 4)
- `--output-dir PATH` - Output directory for generated files (default: current directory)
- `--config-file PATH` - Path to CJA configuration file (default: myconfig.json)
- `--continue-on-error` - Continue processing remaining data views if one fails
- `--log-level LEVEL` - Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO or CJA_LOG_LEVEL env var)
- `--production` - Enable production mode (minimal logging for 5-10% performance gain)
- `--dry-run` - Validate configuration and connectivity without generating reports
- `--quiet, -q` - Quiet mode: suppress all output except errors and final summary
- `--list-dataviews` - List all accessible data views and exit (no data view ID required)
- `--skip-validation` - Skip data quality validation for faster processing (20-30% faster)
- `--sample-config` - Generate a sample configuration file and exit
- `-h, --help` - Show help message and exit

**Environment Variables:**
- `CJA_LOG_LEVEL` - Default log level (overridden by `--log-level` argument)

**Get Help:**
```bash
# Show version
uv run python cja_sdr_generator.py --version

# Show help
uv run python cja_sdr_generator.py --help
```

### 4.3 Running the Script

#### **Single Data View Processing**

```bash
# Basic usage with default settings
uv run python cja_sdr_generator.py dv_677ea9291244fd082f02dd42

# With custom output directory
uv run python cja_sdr_generator.py dv_12345 --output-dir ./reports

# With custom config file
uv run python cja_sdr_generator.py dv_12345 --config-file ./prod_config.json

# With debug logging
uv run python cja_sdr_generator.py dv_12345 --log-level DEBUG

# Production mode (5-10% faster, minimal logging)
uv run python cja_sdr_generator.py dv_12345 --production

# Dry-run to validate config and connectivity without generating reports
uv run python cja_sdr_generator.py dv_12345 --dry-run

# Quiet mode (suppress output except errors and final summary)
uv run python cja_sdr_generator.py dv_12345 --quiet

# Skip data quality validation (20-30% faster)
uv run python cja_sdr_generator.py dv_12345 --skip-validation

# Using environment variable for log level
export CJA_LOG_LEVEL=WARNING
uv run python cja_sdr_generator.py dv_12345
```

#### **Discovery and Setup Commands**

```bash
# List all accessible data views (helps discover correct IDs)
uv run python cja_sdr_generator.py --list-dataviews

# Generate a sample configuration file
uv run python cja_sdr_generator.py --sample-config
```

#### **Multiple Data Views (Automatic Batch Mode)**

When you provide multiple data view IDs, the script automatically enables parallel batch processing:

```bash
# Automatically triggers batch/parallel processing
uv run python cja_sdr_generator.py dv_12345 dv_67890 dv_abcde

# Explicitly use batch mode (same result)
uv run python cja_sdr_generator.py --batch dv_12345 dv_67890 dv_abcde
```

**Note**: The `--batch` flag is optional when processing multiple data views. The script automatically detects multiple data views and uses parallel processing for optimal performance.

#### **Batch Processing Configuration**

Customize parallel processing behavior:

```bash
# Batch with default 4 workers
uv run python cja_sdr_generator.py --batch dv_12345 dv_67890 dv_abcde

# Custom worker count (conservative for shared API)
uv run python cja_sdr_generator.py --batch dv_12345 dv_67890 --workers 2

# Aggressive parallelism (dedicated infrastructure)
uv run python cja_sdr_generator.py --batch dv_* --workers 8

# Continue on errors (resilient mode)
uv run python cja_sdr_generator.py --batch dv_* --continue-on-error

# Full production example
uv run python cja_sdr_generator.py --batch \
  dv_prod_12345 dv_staging_67890 dv_dev_abcde \
  --workers 4 \
  --output-dir ./sdr_reports \
  --continue-on-error \
  --log-level WARNING
```

#### **Reading Data Views from File**

```bash
# Create file with data view IDs
cat > dataviews.txt <<EOF
dv_production_12345
dv_staging_67890
dv_development_abcde
EOF

# Process all data views from file
uv run python cja_sdr_generator.py --batch $(cat dataviews.txt)
```

#### **Using Python Directly**

If you prefer not to use `uv`:

```bash
# Activate virtual environment first
source .venv/bin/activate  # macOS/Linux
# or
.venv\Scripts\activate     # Windows

# Run the script
python cja_sdr_generator.py dv_12345
```

**Expected Console Output (Single Data View):**
```
Processing data view: dv_677ea9291244fd082f02dd42

2026-01-07 10:30:15 - INFO - Logging initialized. Log file: logs/SDR_Generation_dv_677ea9291244fd082f02dd42_20260107_103015.log
============================================================
INITIALIZING CJA CONNECTION
============================================================
2026-01-07 10:30:16 - INFO - Validating configuration file: myconfig.json
2026-01-07 10:30:17 - INFO - âœ“ API connection successful! Found 85 data view(s)
============================================================
VALIDATING DATA VIEW
============================================================
2026-01-07 10:30:19 - INFO - âœ“ Data view validated successfully!
2026-01-07 10:30:19 - INFO -   Name: Production Analytics
2026-01-07 10:30:19 - INFO -   ID: dv_677ea9291244fd082f02dd42
============================================================
Starting optimized data fetch operations
============================================================
2026-01-07 10:30:20 - INFO - Successfully fetched 150 metrics
2026-01-07 10:30:22 - INFO - Successfully fetched 75 dimensions
============================================================
GENERATING EXCEL FILE
============================================================
2026-01-07 10:30:25 - INFO - âœ“ SDR generation complete! File saved as: CJA_DataView_Production_Analytics_dv_677ea9291244fd082f02dd42_SDR.xlsx
```

**Expected Console Output (Batch Mode):**
```
Processing 3 data view(s) in batch mode with 4 workers...

2026-01-07 10:30:00 - INFO - ============================================================
2026-01-07 10:30:00 - INFO - BATCH PROCESSING START
2026-01-07 10:30:00 - INFO - ============================================================
2026-01-07 10:30:00 - INFO - Data views to process: 3
2026-01-07 10:30:00 - INFO - Parallel workers: 4
2026-01-07 10:30:00 - INFO - ============================================================
2026-01-07 10:30:15 - INFO - âœ“ dv_12345: SUCCESS (14.5s)
2026-01-07 10:30:16 - INFO - âœ“ dv_67890: SUCCESS (15.2s)
2026-01-07 10:30:18 - INFO - âœ“ dv_abcde: SUCCESS (16.1s)

============================================================
BATCH PROCESSING SUMMARY
============================================================
Total data views: 3
Successful: 3
Failed: 0
Success rate: 100.0%
Total duration: 18.5s
Average per data view: 6.2s

Successful Data Views:
  âœ“ dv_12345         Production Analytics        14.5s
  âœ“ dv_67890         Development Analytics       15.2s
  âœ“ dv_abcde         Testing Analytics           16.1s
============================================================
Throughput: 9.7 data views per minute
============================================================
```

### 4.4 Output Files

**Excel Workbook(s):**
- Filename Format: `CJA_DataView_[Name]_[ID]_SDR.xlsx`
- Location: Specified by `--output-dir` (default: current directory)
- Size: Typically 1-10 MB depending on data view size
- Multiple Files: One Excel file per data view processed

**Log Files:**

*Single Mode:*
- Filename: `SDR_Generation_[DataViewID]_[Timestamp].log`
- Location: `logs/` subdirectory (created automatically)
- Contains: Complete execution trace for one data view

*Batch Mode:*
- Main Log: `SDR_Batch_Generation_[Timestamp].log`
- Location: `logs/` subdirectory
- Contains: Batch processing summary, all worker outputs, success/failure tracking

**Output Directory Structure:**
```
cja-auto-sdr-2026/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ SDR_Generation_dv_12345_20260107_103015.log    # Single mode
â”‚   â””â”€â”€ SDR_Batch_Generation_20260107_120000.log        # Batch mode
â”œâ”€â”€ CJA_DataView_Production_Analytics_dv_12345_SDR.xlsx
â”œâ”€â”€ CJA_DataView_Development_Analytics_dv_67890_SDR.xlsx
â””â”€â”€ CJA_DataView_Testing_Analytics_dv_abcde_SDR.xlsx
```

Or with custom output directory:
```
./reports/
â”œâ”€â”€ CJA_DataView_Production_Analytics_dv_12345_SDR.xlsx
â”œâ”€â”€ CJA_DataView_Development_Analytics_dv_67890_SDR.xlsx
â””â”€â”€ CJA_DataView_Testing_Analytics_dv_abcde_SDR.xlsx
```

---

## 5. Output Structure

### 5.1 Excel Workbook Sheets

#### **Sheet 1: Metadata**
Summary information about the data view and generation process:
- Generation timestamp with timezone
- Data View ID and name
- Total metrics and dimensions count
- Component type breakdown
- Data quality issue summary
- Severity distribution

#### **Sheet 2: Data Quality** (NEW in v3.0)
Comprehensive data quality report with color-coded severity:
- **CRITICAL** (Red): Issues requiring immediate attention
- **HIGH** (Yellow): Important issues affecting functionality
- **MEDIUM** (Green): Moderate issues to address
- **LOW** (Blue): Minor improvements recommended

Columns:
- Severity level
- Category (Duplicates, Missing Fields, Null Values, etc.)
- Type (Metrics/Dimensions)
- Item Name (affected component)
- Issue description
- Detailed explanation

#### **Sheet 3: DataView**
Complete data view configuration details:
- Data view metadata
- Configuration settings
- Applied filters/segments
- Date range settings
- All JSON-formatted for technical reference

#### **Sheet 4: Metrics**
All metrics with full details:
- Metric ID and name
- Type (calculated, standard, currency, etc.)
- Title and description
- Data type
- Formatting rules
- Attribution settings
- Additional metadata (JSON formatted)

#### **Sheet 5: Dimensions**
All dimensions with comprehensive information:
- Dimension ID and name
- Type (string, numeric, time-based, etc.)
- Title and description
- Data type
- Classification settings
- Persistence settings
- Additional metadata (JSON formatted)

### 5.2 Log Files

Log files are stored in the `logs/` directory with detailed information:

```
logs/
â””â”€â”€ SDR_Generation_dv_677ea9291244fd082f02dd42_20250105_103015.log
```

**Log Content Includes:**
- Timestamp for each operation
- Success/failure status
- Error messages and stack traces
- API response times
- Data quality findings
- File generation progress
- Execution summary

---

## 6. Data Quality Validation

### 6.1 Validation Checks

The script performs the following automated checks:

1. **Duplicate Detection**
   - Identifies metrics/dimensions with identical names
   - Severity: HIGH
   - Impact: Causes confusion in reporting

2. **Required Fields Validation**
   - Ensures `id`, `name`, and `type` fields are present
   - Severity: CRITICAL
   - Impact: Missing fields prevent proper component usage

3. **Null Value Detection**
   - Finds missing values in critical fields (id, name, title, description)
   - Severity: MEDIUM
   - Impact: Incomplete metadata affects discoverability

4. **Missing Descriptions**
   - Identifies components without descriptions
   - Severity: LOW
   - Impact: Reduces documentation quality

5. **Empty Dataset Check**
   - Detects if API returns no data
   - Severity: CRITICAL
   - Impact: No components available for analysis

6. **Invalid ID Check**
   - Finds components with missing or malformed IDs
   - Severity: HIGH
   - Impact: Components cannot be referenced properly

7. **Field Existence Validation**
   - Verifies expected columns are present in API response
   - Severity: CRITICAL
   - Impact: May indicate API changes or permissions issues

8. **Data Completeness**
   - Overall assessment of data quality
   - Multiple severity levels
   - Impact: Varies by specific issue

### 6.2 Severity Levels

**CRITICAL (Red Background)**
- Requires immediate attention
- Blocks core functionality
- Examples: Missing required fields, empty datasets, API errors

**HIGH (Yellow Background)**
- Important issues affecting reliability
- Should be addressed soon
- Examples: Duplicate names, invalid IDs, authentication warnings

**MEDIUM (Green Background)**
- Moderate quality issues
- Address when convenient
- Examples: Null values in secondary fields, minor inconsistencies

**LOW (Blue Background)**
- Minor improvements recommended
- Does not affect functionality
- Examples: Missing descriptions, formatting suggestions

### 6.3 Understanding Issues

Each data quality issue includes:
- **Category**: Type of problem (Duplicates, Missing Fields, etc.)
- **Type**: Whether it affects Metrics or Dimensions
- **Item Name**: Specific component(s) affected
- **Issue**: Clear description of the problem
- **Details**: Additional context and affected items

**Example Issues:**

```
Severity: HIGH
Category: Duplicates
Type: Metrics
Item Name: Page Views
Issue: Duplicate name found 2 times
Details: This metrics name appears 2 times in the data view
```

```
Severity: MEDIUM
Category: Null Values
Type: Dimensions
Item Name: 5 items
Issue: Null values in "description" field
Details: 5 item(s) missing description. Items: eVar1, eVar5, prop3...
```

---

## 7. Error Handling

### 7.1 Connection Errors

**Problem**: Cannot connect to CJA API

**Possible Causes:**
- Invalid credentials in `myconfig.json`
- Network connectivity issues
- Adobe service outage
- Expired authentication tokens

**Solutions:**
1. Verify all fields in `myconfig.json` are correct
2. Check network connectivity
3. Confirm private key file exists and is readable
4. Check Adobe Status page for service issues
5. Regenerate credentials in Adobe I/O Console

**Log Output:**
```
CRITICAL - CJA INITIALIZATION FAILED
CRITICAL - Failed to initialize CJA connection: Authentication failed
CRITICAL - Troubleshooting steps:
CRITICAL - 1. Verify your configuration file exists and is valid JSON
CRITICAL - 2. Check that all authentication credentials are correct
```

### 7.2 Authentication Issues

**Problem**: Authentication fails with valid credentials

**Possible Causes:**
- Incorrect organization ID
- Private key doesn't match integration
- Insufficient API permissions
- Integration not enabled for CJA

**Solutions:**
1. Verify `org_id` matches Adobe I/O Console
2. Re-download private key from integration
3. Check integration has "Customer Journey Analytics" API enabled
4. Verify product profile includes CJA access

### 7.3 Data View Problems

**Problem**: Data view validation fails

**Error Messages:**
```
ERROR - Data view 'dv_12345' returned empty response
INFO - You have access to 5 data view(s):
INFO -   1. Production Analytics (ID: dv_abc123)
INFO -   2. Development Analytics (ID: dv_def456)
```

**Solutions:**
1. Copy the correct data view ID from the log output
2. Verify you have access permissions to the data view
3. Confirm the data view exists in your organization
4. Check with admin if you need access granted

### 7.4 Network Errors and Automatic Retry

The script automatically retries API calls when transient network errors occur.

**Retry Configuration (Default):**
```
Max Retries:      3 attempts after initial failure
Base Delay:       1.0 seconds
Max Delay:        30.0 seconds (cap)
Backoff Formula:  delay = min(base_delay * 2^attempt, max_delay)
Jitter:           Enabled (Â±50% randomization)
```

**Retryable Errors:**
- `ConnectionError` - Network connectivity issues
- `TimeoutError` - Request timeouts
- `OSError` - Other network-related errors

**Example Retry Sequence:**
```
Attempt 1: API call fails (ConnectionError)
  â†’ Wait ~1.0s (with jitter: 0.5-1.5s)
Attempt 2: API call fails (ConnectionError)
  â†’ Wait ~2.0s (with jitter: 1.0-3.0s)
Attempt 3: API call fails (ConnectionError)
  â†’ Wait ~4.0s (with jitter: 2.0-6.0s)
Attempt 4: API call fails
  â†’ Error raised, processing stops for this data view
```

**Log Output During Retry:**
```
WARNING - Attempt 1/4 failed for getMetrics: Connection reset by peer. Retrying in 1.2s...
WARNING - Attempt 2/4 failed for getMetrics: Connection reset by peer. Retrying in 2.4s...
INFO - Successfully fetched 150 metrics
```

**Non-Retryable Errors** (fail immediately):
- `ValueError` - Invalid parameters
- `KeyError` - Missing data
- `AttributeError` - API method not available
- Authentication errors

### 7.5 Troubleshooting Guide

#### Script Won't Start

**Check Python Version:**

```bash
python --version  # Should be 3.14 or higher
```

**Check uv Installation:**

```bash
uv --version
```

**Verify Virtual Environment:**

```bash
# List installed packages
uv pip list

# Should show cjapy, pandas, xlsxwriter
```

**Check Configuration File:**

```bash
# Verify file exists
ls myconfig.json

# Validate JSON syntax
python -c "import json; json.load(open('myconfig.json'))"
```

#### Dependency Issues

**Problem**: Module not found errors

**Solutions:**

```bash
# Reinstall all dependencies
uv sync --reinstall

# Or manually install missing package
uv add package-name

# Verify installation
uv pip list | grep package-name
```

**Problem**: Version conflicts

**Solutions:**

```bash
# Check for conflicts
uv pip check

# Update specific package
uv add --upgrade package-name

# Regenerate lock file
rm uv.lock
uv sync
```

#### Virtual Environment Issues

**Problem**: Wrong Python version or packages

**Solutions:**

```bash
# Remove and recreate virtual environment
rm -rf .venv
uv sync

# Or specify Python version
uv venv --python 3.14
uv sync
```

#### Empty Output File

**Check Log File For:**

- "No metrics returned from API"
- "No dimensions returned from API"
- Possible data view has no components configured

**Solutions:**

- Verify data view has components
- Check API permissions include read access
- Try different data view

#### File Permission Errors

**Error:** `PermissionError: [Errno 13] Permission denied`

**Cause:** Output file is open in Excel or locked

**Solution:** Close the Excel file and re-run script

#### CLI Argument Errors

**Error:** `error: the following arguments are required: DATA_VIEW_ID`

**Cause:** No data view IDs provided as arguments

**Solution:** Provide at least one data view ID:

```bash
# Correct usage
uv run python cja_sdr_generator.py dv_12345

# For help
uv run python cja_sdr_generator.py --help
```

**Error:** `Invalid data view ID format: invalid_id, test123`

**Cause:** Data view IDs don't start with `dv_`

**Solution:** Use properly formatted data view IDs:

```bash
# Wrong
uv run python cja_sdr_generator.py 12345 test

# Correct
uv run python cja_sdr_generator.py dv_12345 dv_67890
```

**Error:** No module named 'argparse'

**Cause:** Using Python < 3.2 (very unlikely)

**Solution:** Update Python or reinstall dependencies:

```bash
python --version  # Should be 3.14+
uv sync --reinstall
```

#### Batch Processing Issues

**Problem:** Batch processing slower than expected

**Possible Causes:**
- Too many workers causing API rate limiting
- Not using `--batch` flag (processes sequentially)
- Network bottleneck

**Solutions:**
```bash
# Ensure --batch flag is used
uv run python cja_sdr_generator.py --batch dv_1 dv_2 dv_3

# Reduce workers if hitting rate limits
uv run python cja_sdr_generator.py --batch dv_* --workers 2

# Check logs for specific errors
cat logs/SDR_Batch_Generation_*.log
```

**Problem:** Some data views fail in batch mode

**Solution:** Use `--continue-on-error` to process all despite failures:

```bash
uv run python cja_sdr_generator.py --batch \
  dv_1 dv_2 dv_3 \
  --continue-on-error
```

Check the batch summary to see which failed and why.

#### Slow Performance

**Normal Duration:** 30-60 seconds for typical data view

**If Slower:**

- Large data views (200+ components) take longer
- Network latency affects API calls
- Check log file for which operation is slow

**Performance Tips:**

```bash
# Run with Python optimization
uv run python -O cja_sdr_generator.py

# Check network latency
ping adobe.io
```

---

## 8. Use Cases

### Comprehensive Implementation Audit
Quickly understand the breadth and depth of your CJA setup:
- Total metrics and dimensions available
- Component type distribution
- Configuration completeness
- Data quality status

**Best For:** Quarterly reviews, new team member onboarding

### Implementation Verification
Ensure your CJA implementation matches planning documents:
- Compare against original SDR
- Validate naming conventions
- Verify all planned metrics exist
- Identify configuration drift

**Best For:** Post-implementation validation, compliance audits

### Data Quality Assurance
Maintain high-quality analytics configuration:
- Identify duplicate components
- Find missing descriptions
- Validate metadata completeness
- Track quality trends over time

**Best For:** Ongoing maintenance, quality improvement initiatives

### Team Onboarding
Assist new team members in understanding CJA setup:
- Provide complete component reference
- Document available metrics/dimensions
- Share data view configuration
- Explain component relationships

**Best For:** Training, documentation, knowledge transfer

### Change Management
Document configuration before and after changes:
- Baseline current configuration
- Compare versions over time
- Track component additions/removals
- Audit change impact

**Best For:** Release management, change control processes

### Multi-Environment Comparison
Compare configurations across environments:
- Generate SDRs for dev, staging, production
- Identify configuration differences
- Ensure consistency across environments
- Plan promotion strategies

**Best For:** DevOps, environment management

### Compliance Documentation
Generate audit-ready documentation:
- Complete component inventory
- Metadata completeness tracking
- Data quality reporting
- Timestamped generation logs

**Best For:** SOC2, ISO, internal audit requirements

### Migration Planning
Prepare for migrations or upgrades:
- Document current state comprehensively
- Identify components to migrate
- Plan migration sequence
- Validate post-migration configuration

**Best For:** Platform migrations, major version upgrades

---

## 9. Best Practices

### Dependency Management with uv

**Lock File Management**
- Commit `uv.lock` to version control for reproducible builds
- Update dependencies regularly: `uv sync --upgrade`
- Test updates in development before production

**Virtual Environment**
- Always use the project's virtual environment
- Don't install packages globally
- Use `uv run` to automatically activate environment

**Adding New Dependencies**
```bash
# Add new package
uv add package-name

# Add with version constraint
uv add "package-name>=1.0.0"

# Add development dependency
uv add --dev pytest
```

### Regular Execution
- Generate SDRs monthly for trending
- Run after significant configuration changes
- Schedule quarterly comprehensive audits
- Maintain version history of output files

**Automated Scheduling:**

**Linux/macOS (cron):**
```bash
# Edit crontab
crontab -e

# Run single data view every Monday at 9 AM
0 9 * * 1 cd /path/to/cja-auto-sdr-2026 && /path/to/uv run python cja_sdr_generator.py dv_production_12345

# Batch process multiple data views daily at 2 AM
0 2 * * * cd /path/to/cja-auto-sdr-2026 && /path/to/uv run python cja_sdr_generator.py --batch dv_prod_1 dv_prod_2 dv_prod_3 --output-dir /reports/$(date +\%Y\%m\%d) --continue-on-error

# Process from file weekly
0 0 * * 0 cd /path/to/cja-auto-sdr-2026 && /path/to/uv run python cja_sdr_generator.py --batch $(cat dataviews.txt) --workers 8 --continue-on-error
```

**Windows (Task Scheduler):**
```powershell
# Single data view task
$action = New-ScheduledTaskAction -Execute "uv" -Argument "run python cja_sdr_generator.py dv_production_12345" -WorkingDirectory "C:\path\to\cja-auto-sdr-2026"
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek Monday -At 9am
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "CJA SDR Generation - Production"

# Batch processing task
$action = New-ScheduledTaskAction -Execute "uv" -Argument "run python cja_sdr_generator.py --batch dv_prod_1 dv_prod_2 dv_prod_3 --workers 4 --continue-on-error" -WorkingDirectory "C:\path\to\cja-auto-sdr-2026"
$trigger = New-ScheduledTaskTrigger -Daily -At 2am
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "CJA SDR Batch Generation"
```

### Data Quality Management
- Review and address CRITICAL issues immediately
- Create tickets for HIGH severity items
- Schedule MEDIUM/LOW items for maintenance windows
- Track resolution of recurring issues

### Documentation
- Store SDR files in shared documentation repository
- Include generation date in version control
- Link to relevant project documentation
- Share with stakeholders and team members

### Version Control
- Keep log files for audit trails
- Archive historical SDRs
- Track changes between versions
- Document reasons for configuration changes

**Git Best Practices:**
```bash
# Add to .gitignore
echo "myconfig.json" >> .gitignore
echo "*.key" >> .gitignore
echo "*.pem" >> .gitignore
echo "logs/" >> .gitignore
echo "*.xlsx" >> .gitignore

# Commit project files
git add pyproject.toml uv.lock
git commit -m "Update dependencies"
```

### Security
- Never commit `myconfig.json` to version control
- Rotate credentials periodically
- Use service accounts for automated runs
- Restrict access to sensitive data views
- Store private keys securely (use key management systems)

### Performance Optimization

**Batch Processing Best Practices:**
- Use `--batch` mode for processing 2+ data views
- Adjust `--workers` based on infrastructure:
  - Shared API (rate limits): 2 workers
  - Balanced (default): 4 workers
  - Dedicated infrastructure: 8+ workers
- Run during off-peak hours if possible
- Use `--continue-on-error` for resilient batch operations
- Monitor execution time trends via batch summaries

**Worker Optimization Table:**

| Workers | Throughput (10 views) | Best For |
|---------|----------------------|----------|
| 1 | ~350s (sequential) | Testing, debugging |
| 2 | ~175s (2x faster) | Shared API, conservative |
| 4 | ~87s (4x faster) | Default, balanced |
| 8 | ~44s (8x faster) | Dedicated infrastructure |

### Automation with CLI

**Create Automation Scripts:**

Create `scripts/generate_production_sdr.sh`:
```bash
#!/bin/bash
cd "$(dirname "$0")/.."

# Single production data view
uv run python cja_sdr_generator.py dv_production_12345 \
  --output-dir ./reports/production \
  --log-level WARNING
```

Create `scripts/generate_all_environments.sh`:
```bash
#!/bin/bash
cd "$(dirname "$0")/.."

# Batch process all environments
uv run python cja_sdr_generator.py --batch \
  dv_production_12345 \
  dv_staging_67890 \
  dv_development_abcde \
  --workers 4 \
  --output-dir ./reports/$(date +%Y%m%d) \
  --continue-on-error \
  --log-level INFO
```

Make executable:
```bash
chmod +x scripts/*.sh
```

**Windows PowerShell Script:**

Create `scripts/generate_all_environments.ps1`:
```powershell
Set-Location $PSScriptRoot\..

# Batch process all environments
uv run python cja_sdr_generator.py --batch `
  dv_production_12345 `
  dv_staging_67890 `
  dv_development_abcde `
  --workers 4 `
  --output-dir "./reports/$(Get-Date -Format 'yyyyMMdd')" `
  --continue-on-error `
  --log-level INFO
```

**Process from Data View List File:**

Create `dataviews.txt`:
```
dv_production_main
dv_production_eu
dv_production_apac
dv_staging_main
dv_development_main
```

Create `scripts/generate_from_file.sh`:
```bash
#!/bin/bash
cd "$(dirname "$0")/.."

# Batch process from file
uv run python cja_sdr_generator.py --batch \
  $(cat dataviews.txt) \
  --workers 8 \
  --output-dir ./reports/batch_$(date +%Y%m%d_%H%M%S) \
  --continue-on-error
```

---

## 10. Testing

The CJA SDR Generator includes a comprehensive automated test suite using pytest.

### Running Tests

**Run all tests:**
```bash
# Using uv (recommended)
uv run pytest

# With verbose output
uv run pytest -v

# With coverage report (requires pytest-cov)
uv add --dev pytest-cov
uv run pytest --cov=cja_sdr_generator --cov-report=html --cov-report=term
```

**Run specific test categories:**
```bash
# CLI tests
uv run pytest tests/test_cli.py

# Data quality tests
uv run pytest tests/test_data_quality.py

# Optimized validation tests
uv run pytest tests/test_optimized_validation.py

# Utility function tests
uv run pytest tests/test_utils.py
```

### Test Coverage

The test suite includes **161 comprehensive tests**:

- **CLI Tests** (`test_cli.py`) - 19 tests
  - Command-line argument parsing
  - Data view ID validation
  - Dry-run, quiet, and version flag handling
  - Error handling for invalid inputs

- **Data Quality Tests** (`test_data_quality.py`) - 10 tests
  - Duplicate detection
  - Missing field validation
  - Null value detection
  - Severity classification

- **Optimized Validation Tests** (`test_optimized_validation.py`) - 16 tests
  - Single-pass validation correctness
  - Performance benchmarking
  - Comparison with original validation
  - Edge case handling
  - Vectorized operations validation

- **Utility Tests** (`test_utils.py`) - 14 tests
  - Logging configuration
  - Configuration file validation
  - Filename sanitization
  - Performance tracking

- **Early Exit Tests** (`test_early_exit.py`) - 11 tests
  - Empty DataFrame handling
  - Missing required fields detection
  - Early exit behavior validation
  - Performance impact verification

- **Logging Optimization Tests** (`test_logging_optimization.py`) - 15 tests
  - Production mode behavior
  - Log level filtering
  - Conditional logging
  - Summary logging accuracy

- **Parallel Validation Tests** (`test_parallel_validation.py`) - 8 tests
  - Concurrent validation correctness
  - Thread safety verification
  - Performance benchmarking
  - Error handling in parallel mode

- **Validation Caching Tests** (`test_validation_cache.py`) - 15 tests
  - Cache hit/miss behavior
  - LRU eviction correctness
  - TTL expiration timing
  - Thread safety under load
  - Performance improvement verification

- **Dry-Run Tests** (`test_dry_run.py`) - 12 tests
  - Configuration validation
  - API connection testing
  - Data view accessibility verification
  - Error handling scenarios

- **Retry Tests** (`test_retry.py`) - 21 tests
  - Exponential backoff behavior
  - Jitter randomization
  - Max retries enforcement
  - Retryable vs non-retryable exceptions
  - Function metadata preservation

- **Output Format Tests** (`test_output_formats.py`) - 20 tests
  - Excel, CSV, JSON, HTML output validation
  - Format-specific features
  - Edge case handling
  - Unicode and special characters

- **Additional Output Tests** - 2 tests
  - Cross-format consistency
  - Large dataset handling

### Test Structure

```
tests/
â”œâ”€â”€ __init__.py                      # Test package initialization
â”œâ”€â”€ conftest.py                      # Pytest fixtures and shared configuration
â”œâ”€â”€ test_cli.py                      # Command-line interface tests (15 tests)
â”œâ”€â”€ test_data_quality.py             # Data quality validation tests (10 tests)
â”œâ”€â”€ test_dry_run.py                  # Dry-run mode tests (12 tests)
â”œâ”€â”€ test_optimized_validation.py     # Optimized validation tests (16 tests)
â”œâ”€â”€ test_utils.py                    # Utility function tests (14 tests)
â”œâ”€â”€ test_early_exit.py               # Early exit optimization tests (11 tests)
â”œâ”€â”€ test_logging_optimization.py     # Logging optimization tests (15 tests)
â”œâ”€â”€ test_parallel_validation.py      # Parallel validation tests (8 tests)
â”œâ”€â”€ test_validation_cache.py         # Validation caching tests (15 tests)
â”œâ”€â”€ test_output_formats.py           # Output format tests (20 tests)
â””â”€â”€ README.md                        # Detailed testing documentation
```

### Writing New Tests

Follow pytest conventions:
- Test files: `test_*.py`
- Test classes: `Test*`
- Test functions: `test_*`

Example:
```python
def test_example_functionality():
    """Test description"""
    # Arrange
    input_data = "test_input"

    # Act
    result = function_under_test(input_data)

    # Assert
    assert result == expected_output
```

### Continuous Integration

Integrate tests into your CI/CD pipeline:

```yaml
# GitHub Actions example
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.14'
      - name: Install dependencies
        run: |
          pip install uv
          uv sync
      - name: Run tests
        run: uv run pytest
```

For more details, see [tests/README.md](tests/README.md).

---

## 11. Performance Optimizations

### 11.1 Optimized Data Quality Validation

Version 3.0 includes significant performance improvements to data quality validation through single-pass DataFrame scanning and vectorized operations.

#### Architecture Improvements

**Before Optimization:**
```python
# Sequential checks requiring multiple DataFrame scans
check_empty_dataframe(metrics)      # Scan 1
check_required_fields(metrics)      # Scan 2
check_duplicates(metrics)           # Scan 3
check_null_values(metrics)          # Scans 4-7 (loops through fields)
check_missing_descriptions(metrics) # Scan 8
check_id_validity(metrics)          # Scan 9

# Total: 9 complete DataFrame scans
```

**After Optimization:**
```python
# Single-pass validation with vectorized operations
check_all_quality_issues_optimized(
    metrics, 'Metrics',
    REQUIRED_FIELDS, CRITICAL_FIELDS
)

# Total: 1 optimized DataFrame scan (89% reduction)
```

#### Performance Metrics

**Dataset Size Impact:**
```
Small Data View (50 components):
  Before: 0.5s validation
  After:  0.5s validation
  Impact: Marginal (logging overhead dominates)

Medium Data View (150 components):
  Before: 1.8s validation
  After:  1.0s validation
  Impact: 44% faster âš¡

Large Data View (225+ components):
  Before: 2.5s validation
  After:  1.2s validation
  Impact: 48% faster âš¡

Enterprise Data View (500+ components):
  Before: 5.2s validation
  After:  2.8s validation
  Impact: 46% faster âš¡
```

**Batch Processing Impact:**
```
10 Data Views (avg 200 components each):
  Before: ~25s total validation time
  After:  ~12s total validation time
  Savings: 13 seconds per batch run

100 Data Views (monthly automation):
  Before: ~4.2 minutes validation
  After:  ~2.0 minutes validation
  Savings: 2.2 minutes per month
```

#### Technical Implementation

**Key Optimizations:**

1. **Single-Pass Validation**
   - Combines 6 validation checks into one DataFrame traversal
   - Reduces memory allocations and garbage collection overhead
   - Better CPU cache utilization

2. **Vectorized Operations**
   ```python
   # Before: Loop through fields
   for field in critical_fields:
       null_count = df[field].isna().sum()

   # After: Single vectorized operation
   null_counts = df[available_fields].isna().sum()
   ```

3. **Performance Tracking**
   - Built-in timing metrics via PerformanceTracker
   - Logs validation duration for monitoring
   - Enables production performance analysis

#### Monitoring Performance

**Check validation performance in logs:**

```bash
# View validation timing
grep "Data Quality Validation completed" logs/*.log

# Example output:
# â±ï¸  Data Quality Validation completed in 1.23s
```

**Performance Summary in Logs:**
```
============================================================
PERFORMANCE SUMMARY
============================================================
Parallel API Fetch                 :   3.45s ( 32.1%)
Data Quality Validation            :   1.23s ( 11.4%)
Processing data for Excel export   :   2.87s ( 26.7%)
Generating Excel file              :   3.20s ( 29.8%)
============================================================
Total Execution Time               :  10.75s
============================================================
```

#### Backward Compatibility

The optimization is **100% backward compatible:**

- Original validation methods preserved
- Identical validation results
- Same issue structure and format
- No breaking changes
- All existing tests still pass

Both validation approaches are available:
```python
# Option 1: Original sequential validation (preserved for compatibility)
dq_checker.check_duplicates(df, 'Metrics')
dq_checker.check_null_values(df, 'Metrics', fields)
# ... etc

# Option 2: Optimized single-pass validation (recommended)
dq_checker.check_all_quality_issues_optimized(
    df, 'Metrics', required_fields, critical_fields
)
```

The main processing function uses the optimized approach by default for best performance.

#### Benefits Summary

| Benefit | Impact |
|---------|--------|
| **Reduced DataFrame Scans** | 89% fewer scans (9 â†’ 1) |
| **Faster Validation** | 30-50% faster for 150+ components |
| **Better Scalability** | Performance improves with dataset size |
| **Cleaner Code** | Single method vs 6 separate calls |
| **Performance Tracking** | Built-in timing metrics |
| **Production Ready** | Comprehensive test coverage (16 tests) |

For detailed implementation information, see [OPTIMIZATION_SUMMARY.md](OPTIMIZATION_SUMMARY.md).

### 11.2 Validation Result Caching

Version 3.0 includes intelligent validation result caching for dramatic performance improvements when validating the same data multiple times.

#### What is Validation Caching?

Validation caching stores the results of data quality checks in memory, eliminating redundant validation when processing identical data. The cache uses content-based hashing to detect when data or configuration changes, automatically invalidating stale entries.

**Key Features:**
- **Smart Cache Keys**: Automatically detects data and configuration changes using DataFrame content hashing
- **LRU Eviction**: Prevents unbounded memory growth by removing least recently used entries
- **TTL Support**: Automatic expiration of cached entries after configured time-to-live
- **Thread-Safe**: Compatible with parallel validation using ThreadPoolExecutor
- **Zero Overhead**: No performance impact when cache is disabled (default behavior)
- **Opt-In Design**: Cache must be explicitly enabled via CLI flag

#### When to Use Validation Caching

**Ideal Scenarios:**
- **Development Iterations**: Testing changes with same data repeatedly (80-100% cache hit rate)
- **Batch Processing**: Processing similar data views with repeated structures (30-50% cache hit rate)
- **CI/CD Pipelines**: Automated validation runs on unchanged data (60-80% cache hit rate)
- **Regression Testing**: Validating same test datasets multiple times (90-100% cache hit rate)

**Less Effective For:**
- First-time processing of unique data views
- Constantly changing datasets
- Single-run operations (cache provides no benefit)

#### Enabling Validation Cache

**Basic Usage:**
```bash
# Enable cache with defaults (1000 entries, 1 hour TTL)
uv run python cja_sdr_generator.py dv_12345 --enable-cache
```

**Custom Configuration:**
```bash
# Large batch operations - increase cache size
uv run python cja_sdr_generator.py dv_12345 --enable-cache --cache-size 5000

# Extended TTL for long-running operations (2 hours = 7200 seconds)
uv run python cja_sdr_generator.py dv_12345 --enable-cache --cache-ttl 7200

# Combined with other optimization flags
uv run python cja_sdr_generator.py dv_12345 --enable-cache --production

# Batch processing with cache
uv run python cja_sdr_generator.py --batch dv_1 dv_2 dv_3 --enable-cache --workers 4
```

**CLI Flags:**
- `--enable-cache` - Enable validation result caching (required to use cache)
- `--cache-size N` - Maximum cached entries (default: 1000)
- `--cache-ttl N` - Time-to-live in seconds (default: 3600 = 1 hour)

#### Performance Expectations

**Cache Hit Performance:**
```
Validation Type           Without Cache    With Cache (Hit)    Improvement
Small Data View (50)      0.5s            0.05s               90% faster
Medium Data View (150)    1.0s            0.10s               90% faster
Large Data View (225)     1.2s            0.12s               90% faster
Enterprise (500+)         2.8s            0.28s               90% faster

Typical cache hit: 50-90% faster (70% average)
Cache miss overhead: 1-2% (negligible)
```

**Batch Processing Example:**
```
Scenario: Process 10 data views, 7 have identical structure

Without Cache:
  10 validations Ã— 1.0s = 10.0s total

With Cache (70% hit rate):
  3 validations Ã— 1.0s = 3.0s (misses)
  7 validations Ã— 0.1s = 0.7s (hits)
  Total: 3.7s (63% faster)
```

**Memory Usage:**
- Approximately 1-5 MB per 1000 cached validation results
- LRU eviction prevents unbounded growth
- TTL ensures automatic cleanup

#### Cache Statistics Output

When cache is enabled, detailed statistics appear in the performance summary:

```
============================================================
VALIDATION CACHE STATISTICS
============================================================
Cache Hits:        9
Cache Misses:      3
Hit Rate:          75.0%
Cache Size:        3/1000
Evictions:         0
Estimated Time Saved: 0.44s
============================================================
```

**Statistics Explained:**
- **Cache Hits**: Number of times validation results were retrieved from cache
- **Cache Misses**: Number of times validation had to be performed
- **Hit Rate**: Percentage of requests served from cache
- **Cache Size**: Current entries / Maximum entries
- **Evictions**: Number of entries removed due to LRU policy
- **Estimated Time Saved**: Approximate time saved by cache hits

#### Technical Details

**Cache Key Strategy:**
```
Format: {item_type}:{df_hash}:{config_hash}

Components:
- item_type: 'Metrics' or 'Dimensions'
- df_hash: Content hash using pandas.util.hash_pandas_object
- config_hash: MD5 hash of required_fields and critical_fields

Example: Metrics:1234567890:a1b2c3d4
```

**How It Works:**
1. Before validation, generate cache key from DataFrame content and configuration
2. Check if result exists in cache and hasn't expired (TTL check)
3. On cache hit: Return cached issues immediately (90% faster)
4. On cache miss: Perform validation, store result in cache for future use
5. When cache is full: Evict least recently used entry (LRU policy)

**DataFrame Hashing:**
- Uses `pandas.util.hash_pandas_object()` for efficient content hashing
- Hashing overhead: 1-2ms for 1000 rows (negligible)
- Detects any data changes (added rows, modified values, column changes)

**Thread Safety:**
- All cache operations protected by `threading.Lock()`
- Safe for concurrent access in parallel validation mode
- No race conditions or data corruption

#### Best Practices

**When to Enable Cache:**
```bash
# Good: Development iterations with same test data
uv run python cja_sdr_generator.py dv_test --enable-cache

# Good: Batch processing similar data views
uv run python cja_sdr_generator.py --batch dv_* --enable-cache --workers 4

# Good: CI/CD pipeline validations
uv run python cja_sdr_generator.py dv_staging --enable-cache --cache-ttl 7200
```

**When NOT to Enable Cache:**
```bash
# Not beneficial: Single unique data view (no repeated validations)
uv run python cja_sdr_generator.py dv_unique

# Not beneficial: Constantly changing data (low hit rate)
uv run python cja_sdr_generator.py dv_live_data --enable-cache
```

**Cache Size Guidelines:**
- **Small Operations**: 100-500 entries (single data view, development)
- **Medium Operations**: 500-2000 entries (batch processing, CI/CD)
- **Large Operations**: 2000-5000 entries (extensive batch processing)

**TTL Guidelines:**
- **Short TTL (1800s/30min)**: Frequently changing data, development
- **Default TTL (3600s/1hr)**: Standard operations, balanced approach
- **Long TTL (7200s/2hr)**: Stable data, extended batch operations

#### Monitoring Cache Effectiveness

**Check cache statistics in logs:**
```bash
# View cache performance
grep "VALIDATION CACHE STATISTICS" logs/*.log -A 10

# Example output shows:
# Hit Rate: 75.0% - Good cache effectiveness
# Hit Rate: 20.0% - Poor cache effectiveness, consider disabling
# Hit Rate: 95.0% - Excellent cache effectiveness
```

**Interpreting Hit Rates:**
- **80-100%**: Excellent - Cache is highly effective
- **50-80%**: Good - Significant performance benefit
- **20-50%**: Moderate - Some benefit, consider if worth enabling
- **0-20%**: Poor - Cache overhead likely exceeds benefit, disable

#### Backward Compatibility

Cache is **100% backward compatible:**
- **Opt-In Only**: Must be explicitly enabled with `--enable-cache` flag
- **Default Behavior**: Without flag, no caching occurs (zero overhead)
- **No Breaking Changes**: All existing scripts work identically
- **API Compatible**: DataQualityChecker accepts optional cache parameter
- **Same Results**: Cached and non-cached validation produce identical issues

**Example - Identical Behavior:**
```bash
# Without cache (original behavior)
uv run python cja_sdr_generator.py dv_12345

# With cache (new behavior, must opt-in)
uv run python cja_sdr_generator.py dv_12345 --enable-cache
```

Both commands produce identical validation results, but the cached version will be faster on repeated executions.

#### Troubleshooting

**Cache Not Improving Performance:**
- Check hit rate in cache statistics (should be > 20%)
- Verify you're processing similar/identical data
- Confirm cache isn't being cleared between runs (TTL not too short)
- Ensure cache size is large enough for your workload

**High Memory Usage:**
- Reduce `--cache-size` value
- Shorten `--cache-ttl` to expire entries sooner
- Check cache statistics for number of entries
- Monitor evictions (high evictions = cache too small)

**Unexpected Cache Misses:**
- Data has changed (even minor changes invalidate cache)
- Configuration parameters changed (required_fields, critical_fields)
- TTL expired (entries removed after time limit)
- Cache was cleared or process restarted

---

## 12. Support and Logging

### Log File Location

All logs are stored in the `logs/` directory:
```
logs/
â”œâ”€â”€ SDR_Generation_dv_677ea9291244fd082f02dd42_20250105_103015.log
â”œâ”€â”€ SDR_Generation_dv_677ea9291244fd082f02dd42_20250105_140522.log
â””â”€â”€ SDR_Generation_dv_abc123def456_20250105_160330.log
```

### Log Levels

- **INFO**: Normal operations and progress updates
- **WARNING**: Non-critical issues that don't stop execution
- **ERROR**: Errors that may affect output quality
- **CRITICAL**: Fatal errors that stop execution

### Getting Help

If you encounter issues:

1. **Check the log file** in `logs/` directory for detailed error information
2. **Review the Data Quality sheet** for configuration issues
3. **Verify configuration** in `myconfig.json`
4. **Check prerequisites** are met (Python version, uv installation, libraries)
5. **Consult troubleshooting guide** above

### Reporting Issues

When reporting issues, include:
- Complete error message from console
- Relevant log file entries
- Python version: `python --version`
- uv version: `uv --version`
- Dependency versions: `uv pip list`
- Project info: `cat pyproject.toml`
- Anonymized configuration (remove sensitive data)
- Steps to reproduce

### Environment Information

**Quick diagnostic script:**
```bash
#!/bin/bash
echo "=== System Information ==="
python --version
uv --version
echo ""
echo "=== Project Dependencies ==="
uv pip list
echo ""
echo "=== Project Configuration ==="
cat pyproject.toml
echo ""
echo "=== Recent Logs ==="
ls -lh logs/ | tail -5
```

Save as `diagnose.sh` and run:
```bash
chmod +x diagnose.sh
./diagnose.sh > diagnostic_report.txt
```

### Common Error Solutions

**Error: `ModuleNotFoundError: No module named 'cjapy'`**

```bash
# Solution: Sync dependencies
uv sync

# Or reinstall
uv sync --reinstall
```

**Error: `uv: command not found`**

```bash
# Solution: Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Then sync project
uv sync
```

**Error: `Python version mismatch`**

```bash
# Solution: Recreate with correct Python version
rm -rf .venv
uv venv --python 3.14
uv sync
```

**Error: `Could not find a version that satisfies the requirement`**

```bash
# Solution: Update package index
uv pip install --upgrade pip
uv sync --reinstall
```

---

## 13. Additional Resources

### uv Documentation
- Official uv Documentation: https://github.com/astral-sh/uv
- uv Installation Guide: https://astral.sh/uv
- Python Packaging Guide: https://packaging.python.org/

### CJA API Resources
- Adobe Analytics API Documentation: https://developer.adobe.com/cja-apis/docs/
- cjapy Library Documentation: https://github.com/pitchmuc/cjapy
- Adobe I/O Console: https://console.adobe.io/

### Python Resources
- Python 3.14 Documentation: https://docs.python.org/3.14/
- pandas Documentation: https://pandas.pydata.org/docs/
- xlsxwriter Documentation: https://xlsxwriter.readthedocs.io/

### Project Updates

**Check for Updates:**

```bash
# Update all dependencies
uv sync --upgrade

# Update specific package
uv add --upgrade cjapy
```

**View Dependency Tree:**
```bash
uv pip show cjapy
```

**Export Dependencies:**
```bash
# Export for pip compatibility
uv pip compile pyproject.toml -o requirements.txt
```

---

## Conclusion

This enhanced CJA SDR Generator provides enterprise-grade reliability, comprehensive data quality validation, and detailed logging to ensure your Customer Journey Analytics implementation is well-documented, maintainable, and audit-ready. 

With modern dependency management via `uv` and `pyproject.toml`, the project offers:
- **Fast, reliable installations** with uv's Rust-powered package resolution
- **Reproducible builds** through lock files
- **Easy updates** with simple commands
- **Professional project structure** following Python best practices

The automated quality checks help maintain high standards while the robust error handling ensures reliable execution even in challenging environments.

By automating what could be a time-consuming manual process, you can spend more time on analysis and less on administration, while maintaining confidence in your analytics configuration quality.

### Quick Start Reminder

```bash
# 1. Clone/download project
cd cja-auto-sdr-2026

# 2. Install uv (if needed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 3. Setup project
uv sync

# 4. Configure credentials
# Create myconfig.json with your Adobe credentials

# 5. Run the generator

# Single data view
uv run python cja_sdr_generator.py dv_YOUR_DATA_VIEW_ID

# Or batch process multiple data views (3-4x faster)
uv run python cja_sdr_generator.py --batch dv_ID1 dv_ID2 dv_ID3

# With custom options
uv run python cja_sdr_generator.py --batch \
  dv_ID1 dv_ID2 dv_ID3 \
  --workers 4 \
  --output-dir ./reports \
  --continue-on-error

# 6. Review output
# Check the generated Excel file(s) and logs/ directory
```

### Additional Resources

For detailed batch processing documentation, see [BATCH_PROCESSING_GUIDE.md](BATCH_PROCESSING_GUIDE.md)
